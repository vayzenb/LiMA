{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses an autoencoder to approximate the habituation/dishabituation process\n",
    "An autoencoder is afixed ontop of the pre-trained model \n",
    "LiMA stim are run through it\n",
    "\n",
    "Author: VAYZENB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dir = '/home/vayzenbe/GitHub_Repos/LiMA'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, f'{curr_dir}')\n",
    "import os, argparse\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from LoadFrames import LoadFrames\n",
    "from statistics import mean\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ['Exp1']\n",
    "\n",
    "skel = [['23'],['31_0', '31_50']]\n",
    "SF = ['Skel', 'Bulge']\n",
    "#modelType = ['AlexNet_SN', 'ResNet_SN', 'AlexNet_IN', 'ResNet_IN', 'CorNet_Z', 'CorNet_S','SayCam']\n",
    "modelType = 'skel'\n",
    "batch_num = 10\n",
    "#hab_min = 4 #minimum number of habituation trials to \n",
    "#batch_num = 10 #how many frames to use at a time\n",
    "#exp = ['Exp1']\n",
    "#skel=[['23']]\n",
    "#SF = ['Skel']\n",
    "#modelType = ['ResNet_SN',  'ResNet_IN', 'CorNet_Z', 'CorNet_S','SayCam']\n",
    "\n",
    "#Transformations for ImageNet\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])])\n",
    "inv_normalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],std=[1/0.229, 1/0.224, 1/0.225])\n",
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "hab_min = 3 #minimum number of habituation trials to \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_decoder():\n",
    "    #decoder = nn.Sequential(nn.Conv2d(3,1024,kernel_size=3, stride=2), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2, padding=1), nn.AdaptiveAvgPool2d(1),nn.ReLU(), nn.ConvTranspose2d(1024, 3, 224))\n",
    "    decoder = nn.Sequential(nn.Conv2d(3,1024,kernel_size=3, stride=2), nn.ReLU(), nn.AdaptiveAvgPool2d(1),nn.ReLU(), nn.ConvTranspose2d(1024, 3, 224))\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch, optimizer, loss, file_path):\n",
    "\n",
    "    print('Saving model ...')\n",
    "    #torch.save(model.state_dict(), f'{weights_dir}/cornet_classify_{cond}_{epoch}.pt')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        }, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recon(out_,sk_,sf_, model, stim):\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Show the image\n",
    "    #ax.imshow(im)\n",
    "\n",
    "    out_ = out_.squeeze(0)\n",
    "    out_ = inv_normalize(out_)\n",
    "    out_ = out_.cpu().detach()\n",
    "    ax.imshow(out_.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    #print(f'Results/AE/recon/{model}_{fig_fig_}.png')\n",
    "    plt.savefig(f'{curr_dir}/Results/AE/recon/{model}_{stim}_skel.png', bbox_inches='tight', pad_inches = 0, dpi=150)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 10.92 GiB total capacity; 8.78 GiB already allocated; 83.38 MiB free; 10.13 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c2ad934294e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                     \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 10.92 GiB total capacity; 8.78 GiB already allocated; 83.38 MiB free; 10.13 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "habituate\n",
    "'''\n",
    "\n",
    "\n",
    "for ee in range(0,len(exp)):\n",
    "    hab_data = np.empty(((len(skel[ee]) * len(SF) *len(modelType)),11), dtype = object)\n",
    "    hn = 0\n",
    "\n",
    "\n",
    "    for sk in range(0,len(skel[ee])):\n",
    "        for sf in SF:\n",
    "            torch.cuda.empty_cache() #clear GPU memory\n",
    "            hab_dataset = LoadFrames(f'skels/Figure_{skel[ee][sk]}_{sf}', transform=transform)\n",
    "            trainloader = torch.utils.data.DataLoader(hab_dataset, batch_size=batch_num, shuffle=True, num_workers = 2, pin_memory=True)\n",
    "\n",
    "            early_hab = 0.0\n",
    "            late_hab = []\n",
    "\n",
    "            #Reset decoder for every object (i.e., make it like a fresh hab session)\n",
    "            #Create decoder\n",
    "            decoder = define_decoder()\n",
    "            \n",
    "            decoder.train()\n",
    "\n",
    "            #set up optimzer\n",
    "            optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01)\n",
    "            for ep in range(0,epochs):\n",
    "                train_loss = 0.0 \n",
    "                total_loss =0.0\n",
    "                n = 0\n",
    "                for frames in trainloader:\n",
    "                    frames = frames.cuda()\n",
    "\n",
    "\n",
    "                    optimizer.zero_grad() #zero out gradients from previous epoch\n",
    "\n",
    "                    decode_out = decoder(frames) #Run features through decoder\n",
    "\n",
    "                    loss = criterion(decode_out, frames) #Calculate loss\n",
    "\n",
    "                    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                    loss.backward()\n",
    "                    # perform a single optimization step (parameter update)\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                    train_loss += (loss.item()*frames.size(0))\n",
    "                    n = n +1\n",
    "                    #print(train_loss, loss.item()*frames.size(0), n)\n",
    "\n",
    "                total_loss = train_loss/n\n",
    "\n",
    "                if ep < hab_min:\n",
    "                    early_hab += total_loss #track loss for the first 4 trials\n",
    "                    print(ep, total_loss)\n",
    "                elif ep >= hab_min:\n",
    "                    hab_start = early_hab / hab_min #Determine habituation criterion\n",
    "                    late_hab.append(total_loss) #add current loss to habituation\n",
    "                    hab_end = mean(late_hab[(len(late_hab)-4):len(late_hab)]) #calcualte mean of last 4 hab trials\n",
    "\n",
    "                    print(ep, total_loss, hab_start, hab_end)\n",
    "                    if hab_end < (hab_start/2) and ep >= int(hab_min *2): #test if habituated\n",
    "                        break\n",
    "\n",
    "            hab_data[hn,0] =  'skel'\n",
    "            hab_data[hn,1] =  skel[ee][sk]\n",
    "            hab_data[hn,2] =  sf\n",
    "            hab_data[hn,3] =  ep\n",
    "            hab_data[hn,4] =  hab_start\n",
    "            hab_data[hn,5] =  hab_end\n",
    "\n",
    "            print('Saving model', f'Figure_{skel[ee][sk]}_{sf}', ep, hab_start, hab_end)  \n",
    "             \n",
    "            save_model(decoder, ep, optimizer, loss, f'{curr_dir}/Weights/decoder/{exp[ee]}_skel_Figure_{skel[ee][sk]}_{sf}.pt')\n",
    "            np.save(f'{curr_dir}/Weights/decoder/{exp[ee]}_Summary_Skel.npy', hab_data)\n",
    "            hn = hn + 1\n",
    "            del decoder\n",
    "            gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skel', '23', 'Skel', 6, 31.12451586458418, 3.752286471426487, '23', 'Skel', 'same', 'same', 1.5367094924052556]\n",
      "['skel', '23', 'Skel', 6, 31.12451586458418, 3.752286471426487, '23', 'Bulge', 'same', 'diff', 1.35210948685805]\n",
      "['skel', '23', 'Skel', 6, 31.12451586458418, 3.752286471426487, '31', 'Skel', 'diff', 'same', 1.3344156183302402]\n",
      "['skel', '23', 'Skel', 6, 31.12451586458418, 3.752286471426487, '31', 'Bulge', 'diff', 'diff', 1.3068049252033234]\n",
      "['skel', '23', 'Skel', 6, 31.12451586458418, 3.752286471426487, '26', 'Skel', 'diff', 'same', 1.3108076983027988]\n",
      "['skel', '23', 'Skel', 6, 31.12451586458418, 3.752286471426487, '26', 'Bulge', 'diff', 'diff', 1.2977664066212518]\n",
      "['skel', '23', 'Bulge', 6, 17.710136365559368, 2.9523249187817178, '23', 'Skel', 'same', 'diff', 2.6051981716106334]\n",
      "['skel', '23', 'Bulge', 6, 17.710136365559368, 2.9523249187817178, '23', 'Bulge', 'same', 'same', 2.5346919425107814]\n",
      "['skel', '23', 'Bulge', 6, 17.710136365559368, 2.9523249187817178, '31', 'Skel', 'diff', 'diff', 2.619707571963469]\n",
      "['skel', '23', 'Bulge', 6, 17.710136365559368, 2.9523249187817178, '31', 'Bulge', 'diff', 'same', 2.6454830147100217]\n",
      "['skel', '23', 'Bulge', 6, 17.710136365559368, 2.9523249187817178, '26', 'Skel', 'diff', 'diff', 2.729946637733115]\n",
      "['skel', '23', 'Bulge', 6, 17.710136365559368, 2.9523249187817178, '26', 'Bulge', 'diff', 'same', 2.755282366505036]\n",
      "['skel', '31', 'Skel', 6, 21.237673146857155, 3.196986857801676, '23', 'Skel', 'diff', 'same', 3.394512901348727]\n",
      "['skel', '31', 'Skel', 6, 21.237673146857155, 3.196986857801676, '23', 'Bulge', 'diff', 'diff', 3.2063933544688754]\n",
      "['skel', '31', 'Skel', 6, 21.237673146857155, 3.196986857801676, '31', 'Skel', 'same', 'same', 3.0595335736870766]\n",
      "['skel', '31', 'Skel', 6, 21.237673146857155, 3.196986857801676, '31', 'Bulge', 'same', 'diff', 2.921368863944914]\n",
      "['skel', '31', 'Skel', 6, 21.237673146857155, 3.196986857801676, '26', 'Skel', 'diff', 'same', 2.823345596315684]\n",
      "['skel', '31', 'Skel', 6, 21.237673146857155, 3.196986857801676, '26', 'Bulge', 'diff', 'diff', 2.7232945475139116]\n",
      "['skel', '31', 'Bulge', 6, 24.86173989044295, 3.333798386156559, '23', 'Skel', 'diff', 'diff', 3.0433104522526264]\n",
      "['skel', '31', 'Bulge', 6, 24.86173989044295, 3.333798386156559, '23', 'Bulge', 'diff', 'same', 2.937230117737301]\n",
      "['skel', '31', 'Bulge', 6, 24.86173989044295, 3.333798386156559, '31', 'Skel', 'same', 'diff', 2.8349219793171594]\n",
      "['skel', '31', 'Bulge', 6, 24.86173989044295, 3.333798386156559, '31', 'Bulge', 'same', 'same', 2.739896194442459]\n",
      "['skel', '31', 'Bulge', 6, 24.86173989044295, 3.333798386156559, '26', 'Skel', 'diff', 'diff', 2.660134236017863]\n",
      "['skel', '31', 'Bulge', 6, 24.86173989044295, 3.333798386156559, '26', 'Bulge', 'diff', 'same', 2.585094028711319]\n",
      "['skel', '26', 'Skel', 6, 21.444211651881535, 3.031888923918207, '23', 'Skel', 'diff', 'same', 2.797696366906166]\n",
      "['skel', '26', 'Skel', 6, 21.444211651881535, 3.031888923918207, '23', 'Bulge', 'diff', 'diff', 2.722839461064633]\n",
      "['skel', '26', 'Skel', 6, 21.444211651881535, 3.031888923918207, '31', 'Skel', 'diff', 'same', 2.654878381817114]\n",
      "['skel', '26', 'Skel', 6, 21.444211651881535, 3.031888923918207, '31', 'Bulge', 'diff', 'diff', 2.5902131371114447]\n",
      "['skel', '26', 'Skel', 6, 21.444211651881535, 3.031888923918207, '26', 'Skel', 'same', 'same', 2.5220796283748417]\n",
      "['skel', '26', 'Skel', 6, 21.444211651881535, 3.031888923918207, '26', 'Bulge', 'same', 'diff', 2.4589511696048962]\n",
      "['skel', '26', 'Bulge', 6, 20.985679717527496, 2.9680521475772066, '23', 'Skel', 'diff', 'diff', 2.732420308360209]\n",
      "['skel', '26', 'Bulge', 6, 20.985679717527496, 2.9680521475772066, '23', 'Bulge', 'diff', 'same', 2.669260295277292]\n",
      "['skel', '26', 'Bulge', 6, 20.985679717527496, 2.9680521475772066, '31', 'Skel', 'diff', 'diff', 2.6212792774187585]\n",
      "['skel', '26', 'Bulge', 6, 20.985679717527496, 2.9680521475772066, '31', 'Bulge', 'diff', 'same', 2.571482494118668]\n",
      "['skel', '26', 'Bulge', 6, 20.985679717527496, 2.9680521475772066, '26', 'Skel', 'same', 'diff', 2.525156671181321]\n",
      "['skel', '26', 'Bulge', 6, 20.985679717527496, 2.9680521475772066, '26', 'Bulge', 'same', 'same', 2.4739444500825427]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dishabituate\n",
    "'''\n",
    "\n",
    "\n",
    "for en, ee in enumerate(exp):\n",
    "    \n",
    "    hab_data = np.load(f'{curr_dir}/Weights/decoder/{ee}_Summary_Skel.npy',allow_pickle=True)\n",
    "    \n",
    "\n",
    "#for mm in range(0,1):\n",
    "\n",
    "    #test_data = np.empty(((len(skel[ee]) * len(SF)),hab_data.shape[1]), dtype = object)\n",
    "    \n",
    "    exp_result = []\n",
    "    #habituation index\n",
    "    hn = 0\n",
    "    #load habituated stim\n",
    "    for sk_hab in skel[en]:\n",
    "        for sf_hab in SF:\n",
    "            trial_result = []\n",
    "            torch.cuda.empty_cache() #clear GPU memory\n",
    "            \n",
    "            #Load model\n",
    "            decoder = define_decoder()\n",
    "\n",
    "            #Load checkpoint from fully trained decoder\n",
    "            checkpoint = torch.load(f'{curr_dir}/Weights/decoder/{ee}_skel_Figure_{sk_hab}_{sf_hab}.pt')\n",
    "            #print(f'Weights/decoder/{exp[ee]}_{hab_data[mm,0]}_Figure_{hab_data[mm,1]}_{hab_data[mm,2]}.pt')\n",
    "            decoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "            decoder.eval()\n",
    "            \n",
    "            #load data for dishabituation\n",
    "            for sk_dis in skel[en]:\n",
    "                for sf_dis in SF:\n",
    "                    #load stim for dishab object\n",
    "                    hab_dataset = LoadFrames(f'skels/Figure_{sk_dis}_{sf_dis}', transform=transform)\n",
    "                    trainloader = torch.utils.data.DataLoader(hab_dataset, batch_size=batch_num, shuffle=True, num_workers = 2, pin_memory=True)\n",
    "            \n",
    "\n",
    "                    if sk_hab == sk_dis:\n",
    "                        skel_cat = \"same\"\n",
    "                    else:\n",
    "                        skel_cat = \"diff\"\n",
    "\n",
    "                    if sf_hab == sf_dis:\n",
    "                        sf_cat = \"same\"\n",
    "                    else:\n",
    "                        sf_cat = \"diff\"\n",
    "\n",
    "                    total_loss = []\n",
    "\n",
    "                    for frames in trainloader:\n",
    "                        frames = frames.cuda()\n",
    "\n",
    "                        decode_out = decoder(frames) #Run features through decoder\n",
    "\n",
    "                        loss = criterion(decode_out, frames) #Calculate loss\n",
    "                        train_loss += (loss.item()*frames.size(0))\n",
    "                        n = n +1\n",
    "\n",
    "                        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                        #loss.backward()\n",
    "                        # perform a single optimization step (parameter update)\n",
    "                        #optimizer.step()                        \n",
    "\n",
    "                        #print(train_loss, loss.item()*frames.size(0), n)\n",
    "                    total_loss = train_loss/n\n",
    "                    dishab_trial = hab_data[hn,:6].tolist() + [sk_dis, sf_dis, skel_cat, sf_cat, total_loss]\n",
    "                    print(dishab_trial)\n",
    "                    trial_result.append(dishab_trial)\n",
    "                    exp_result.append(dishab_trial)\n",
    "\n",
    "                    #print(ep, total_loss)\n",
    "\n",
    "                    '''\n",
    "                    if skel_cat == 'same' and sf_cat == 'same':\n",
    "                        save_recon(decode_out, skel_cat, sf_cat, hab_data[hn,0],f'Figure_{hab_data[hn,1]}_{hab_data[hn,2]}')\n",
    "                    '''\n",
    "\n",
    "            #print(trial_result)\n",
    "            hn = hn +1\n",
    "            np.savetxt(f'{curr_dir}/Results/AE/{ee}_{hab_data[hn,0]}_Figure_{hab_data[hn,1]}_{hab_data[hn,2]}_Result.csv', np.array(trial_result), delimiter=',', fmt= '%s')\n",
    "            del decoder\n",
    "            gc.collect()\n",
    "            \n",
    "    \n",
    "    \n",
    "    np.savetxt(f'{curr_dir}/Results/AE/{ee}_Skel_Result.csv', np.array(exp_result), delimiter=',', fmt= '%s')\n",
    "            \n",
    "    #hn = hn + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skel_cat  sf_cat\n",
       "diff      diff      2.521176\n",
       "          same      2.539957\n",
       "same      diff      2.449618\n",
       "          same      2.477809\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array(exp_result), columns = ['model','skel_hab','sf_hab', 'trials', 'hab_start', 'hab_end', 'skel_dishab', 'sf_dishab', 'skel_cat', 'sf_cat', 'loss'])\n",
    "df['loss'] = pd.to_numeric(df['loss'])\n",
    "\n",
    "df.groupby(['skel_cat', 'sf_cat'])['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = '23'\n",
    "sf = 'Skel'\n",
    "model = 'ResNet_SN'\n",
    "ee = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = hab_data[mm,1]\n",
    "sf = hab_data[mm,2]\n",
    "model = hab_data[mm,0]\n",
    "ee = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, in_feat = load_model(model) #load encoder\n",
    "encoder.cuda()\n",
    "encoder.eval()\n",
    "#Create decoder\n",
    "#it gets reset for every object\n",
    "decoder = nn.Sequential(nn.ReLU(), nn.ConvTranspose2d(in_feat, 3, 224))\n",
    "decoder = decoder.cuda()\n",
    "\n",
    "#Load checkpoint from fully trained decoder\n",
    "checkpoint = torch.load(f'Weights/decoder/{exp[ee]}_{model}_Figure_{sk}_{sf}.pt')\n",
    "decoder.load_state_dict(checkpoint)\n",
    "decoder.eval()\n",
    "decoder.train()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hab_dataset = LoadFrames(f'Frames/Figure_{sk}_{sf}', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(hab_dataset, batch_size=batch_num, shuffle=True, num_workers = 0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0.0 \n",
    "n = 0\n",
    "for frames in trainloader:\n",
    "    frames = frames.cuda()\n",
    "\n",
    "\n",
    "    encode_out = encoder(frames) #Get encoder features\n",
    "\n",
    "    optimizer.zero_grad() #zero out gradients from previous epoch\n",
    "\n",
    "    decode_out = decoder(encode_out) #Run features through decoder\n",
    "\n",
    "    loss = criterion(decode_out, frames) #Calculate loss\n",
    "    print((loss.item()*frames.size(0)))\n",
    "    train_loss += (loss.item()*frames.size(0))\n",
    "    n = n +1\n",
    "\n",
    "\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    #loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    #optimizer.step()\n",
    "    optimizer.zero_grad() #zero out gradients from previous epoch\n",
    "\n",
    "\n",
    "\n",
    "    #print(train_loss, loss.item()*frames.size(0), n)\n",
    "print(train_loss/n)\n",
    "\n",
    "#print(ep, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = transforms.Resize((224, 224))\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                 std=[0.229, 0.224, 0.225])\n",
    "#to_tensor = transforms.ToTensor()\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = Image.open(image_name).convert(\"RGB\")\n",
    "    #image = Variable(normalize(to_tensor(scaler(image))).unsqueeze(0))\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sample image\n",
    "IM = image_loader(f'Frames/Figure_{sk}_{sf}/Figure_{sk}_{sf}_150.jpg')\n",
    "IM = IM.cuda()\n",
    "print(IM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Weights/decoder/{exp[ee]}_{model}_Figure_{sk}_{sf}.pt')\n",
    "print(IM.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = encoder(IM)\n",
    "out = decoder(out)\n",
    "loss = criterion(out, IM)\n",
    "print(loss.item()*IM.size(0))\n",
    "#out = out.squeeze(0)\n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on is image preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.squeeze(0)\n",
    "print(out.shape)\n",
    "inv_normalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "output = inv_normalize(out)\n",
    "output = output.cpu().detach()\n",
    "plt.imshow(output.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM = IM.squeeze(0)\n",
    "IM = inv_normalize(IM)\n",
    "IM = IM.cpu().detach()\n",
    "plt.imshow(IM.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on is image preview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
