{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses an autoencoder to approximate the habituation/dishabituation process\n",
    "An autoencoder is afixed ontop of the pre-trained model \n",
    "LiMA stim are run through it\n",
    "\n",
    "Author: VAYZENB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, argparse\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import cornet\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from LoadFrames import LoadFrames\n",
    "from statistics import mean\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = ['Exp1', 'Exp2']\n",
    "\n",
    "skel = [['23','31', '26'],['31_0', '31_50']]\n",
    "SF = ['Skel', 'Bulge']\n",
    "modelType = ['ResNet_SN', 'ResNet_IN', 'CorNet_Z', 'CorNet_S','SayCam']\n",
    "\n",
    "hab_min = 4 #minimum number of habituation trials to \n",
    "batch_num = 10 #how many frames to use at a time\n",
    "#exp = ['Exp2']\n",
    "#skel=[['26']]\n",
    "#SF = ['Bulge']\n",
    "#modelType = ['SayCam']\n",
    "\n",
    "#Transformations for ImageNet\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])])\n",
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "#criterion.cuda()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "#Gets feats for CorNet models\n",
    "def _store_feats(layer, inp, output):\n",
    "    \"\"\"An ugly but effective way of accessing intermediate model features\n",
    "    \"\"\"   \n",
    "    output = output\n",
    "    _model_feats.append(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(modelType_):\n",
    "    #select model to run\n",
    "    if modelType_ == 'AlexNet_IN':\n",
    "        model = torchvision.models.alexnet(pretrained=True)\n",
    "        new_classifier = nn.Sequential(*list(model.classifier.children())[:-2])\n",
    "        model.classifier = new_classifier #replace model classifier with stripped version\n",
    "        layer = \"fc7\"\n",
    "        actNum = 4096\n",
    "        \n",
    "    elif modelType_ == 'ResNet_IN':\n",
    "        model = torchvision.models.resnet50(pretrained=True)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        layer = \"avgpool\"\n",
    "        actNum = 2048\n",
    "                \n",
    "    elif modelType_ == 'AlexNet_SN':\n",
    "        model = torchvision.models.alexnet(pretrained=False)\n",
    "        checkpoint = torch.load('Weights/ShapeNet_AlexNet_Weights.pth.tar')\n",
    "        model.load_state_dict(checkpoint)\n",
    "        new_classifier = nn.Sequential(*list(model.classifier.children())[:-2])\n",
    "        model.classifier = new_classifier #replace model classifier with stripped version\n",
    "        layer = \"fc7\"\n",
    "        actNum = 4096\n",
    "        \n",
    "    elif modelType_ == 'ResNet_SN':\n",
    "        model = torchvision.models.resnet50(pretrained=False)\n",
    "        checkpoint = torch.load('Weights/ShapeNet_ResNet50_Weights.pth.tar')\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        \n",
    "        layer = \"avgpool\"\n",
    "        actNum = 2048\n",
    "    \n",
    "    elif modelType_ == 'CorNet_Z':\n",
    "        model = getattr(cornet, 'cornet_z')\n",
    "        model = model(pretrained=False, map_location='gpu')\n",
    "        checkpoint = torch.load('Weights/cornet_z.pth')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        layer = \"avgpool\"\n",
    "        actNum = 512\n",
    "            \n",
    "        decode_layer = nn.Sequential(*list(model.children())[0][4][:-3])\n",
    "        model = nn.Sequential(*list(model.children())[0][:-1])\n",
    "        model.add_module('4', decode_layer)\n",
    "        \n",
    "        \n",
    "        #try:\n",
    "        #    m = model.module\n",
    "        #except:\n",
    "        #    m = model\n",
    "        #model_layer = getattr(getattr(m, 'decoder'), layer)\n",
    "        #model_layer.register_forward_hook(_store_feats)\n",
    "\n",
    "    elif modelType_ == 'CorNet_S':\n",
    "        model = getattr(cornet, 'cornet_s')\n",
    "        model = model(pretrained=False, map_location='gpu')\n",
    "        checkpoint = torch.load('Weights/cornet_s.pth')\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        layer = \"avgpool\"\n",
    "        actNum = 512        \n",
    "\n",
    "        decode_layer = nn.Sequential(*list(model.children())[0][4][:-3])\n",
    "        model = nn.Sequential(*list(model.children())[0][:-1])\n",
    "        model.add_module('4', decode_layer)\n",
    "        #try:\n",
    "        #    m = model.module\n",
    "        #except:\n",
    "        #    m = model\n",
    "        \n",
    "        #model_layer = getattr(getattr(m, 'decoder'), layer)\n",
    "        #model_layer.register_forward_hook(_store_feats)\n",
    "\n",
    "    elif modelType_ == 'SayCam':\n",
    "        model = torchvision.models.resnext50_32x4d(pretrained=False)\n",
    "        #model = torch.nn.DataParallel(model)\n",
    "        #model.fc = torch.nn.Linear(in_features=2048, out_features=n_out, bias=True)\n",
    "        checkpoint = torch.load('Weights/SayCam_ResNext_Weights.pth.tar')\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "        actNum = 2048\n",
    "\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        \n",
    "    return model, actNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39.39899683935988\n",
      "1 2.1513333805506267\n",
      "2 0.1582532292982984\n",
      "3 0.06823643327750746\n",
      "4 0.05155370115572887 10.444204970621577 0.05155370115572887\n",
      "5 0.04932239647173593 10.444204970621577 0.0504380488137324\n",
      "6 0.04830407776359108 10.444204970621577 0.04830407776359108\n",
      "7 0.04680687976219962 10.444204970621577 0.04899676378831387\n",
      "8 0.043688613117762634 10.444204970621577 0.04703049177882232\n",
      "Saving model SayCam Figure_31_0_Skel 8 10.444204970621577 0.04703049177882232\n",
      "0 50.11749427645437\n",
      "1 2.00041283468806\n",
      "2 0.22781134841422881\n",
      "3 0.10367692183823354\n",
      "4 0.08715447349353664 13.112348845348723 0.08715447349353664\n",
      "5 0.0691110328320534 13.112348845348723 0.07813275316279503\n",
      "6 0.0637040558766814 13.112348845348723 0.0637040558766814\n",
      "7 0.05255942033874171 13.112348845348723 0.06813224563525329\n",
      "8 0.04792697832078462 13.112348845348723 0.058325371842065286\n",
      "Saving model SayCam Figure_31_0_Bulge 8 13.112348845348723 0.058325371842065286\n",
      "0 36.44728674138746\n",
      "1 1.8437067796626398\n",
      "2 0.17753947376003187\n",
      "3 0.09133089135491079\n",
      "4 0.07932628284118348 9.63996597154126 0.07932628284118348\n",
      "5 0.06759466782152172 9.63996597154126 0.07346047533135261\n",
      "6 0.05500728549856332 9.63996597154126 0.05500728549856332\n",
      "7 0.05094740272409493 9.63996597154126 0.06321890972134087\n",
      "8 0.05612284612781819 9.63996597154126 0.05741805054299954\n",
      "Saving model SayCam Figure_31_50_Skel 8 9.63996597154126 0.05741805054299954\n",
      "0 45.47667313967982\n",
      "1 2.15980483393275\n",
      "2 0.23897098665756564\n",
      "3 0.09739018003306081\n",
      "4 0.07581986898496267 11.993209785075798 0.07581986898496267\n",
      "5 0.07742625883510036 11.993209785075798 0.07662306391003151\n",
      "6 0.05766860084728368 11.993209785075798 0.05766860084728368\n",
      "7 0.05383690275372036 11.993209785075798 0.06618790785526676\n",
      "8 0.051043921285459114 11.993209785075798 0.05999392093039088\n",
      "Saving model SayCam Figure_31_50_Bulge 8 11.993209785075798 0.05999392093039088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ee in range(0,len(exp)):\n",
    "    hab_data = np.empty(((len(skel[ee]) * len(SF) *len(modelType)),11), dtype = object)\n",
    "    hn = 0\n",
    "    for mm in range(0, len(modelType)):\n",
    "        \n",
    "        encoder, in_feat = load_model(modelType[mm])\n",
    "        #print(modelType[mm], in_feat)\n",
    "        \n",
    "        #Register forward hook to extract data from avgpool layer\n",
    "        #if modelType[mm] == 'CorNet_Z' or modelType[mm] == 'CorNet_S':\n",
    "        #    try:\n",
    "        #        m = encoder.module\n",
    "        #    except:\n",
    "        #        m = encoder\n",
    "        #    model_layer = getattr(getattr(m, 'decoder'), \"avgpool\")\n",
    "        #    model_layer.register_forward_hook(_store_feats)\n",
    "\n",
    "        encoder = encoder.cuda()\n",
    "        encoder.eval()\n",
    "    \n",
    "        for sk in range(0,len(skel[ee])):\n",
    "            for sf in SF:\n",
    "                torch.cuda.empty_cache() #clear GPU memory\n",
    "                hab_dataset = LoadFrames(f'Frames/Figure_{skel[ee][sk]}_{sf}', transform=transform)\n",
    "                trainloader = torch.utils.data.DataLoader(hab_dataset, batch_size=batch_num, shuffle=True, num_workers = 2, pin_memory=True)\n",
    "\n",
    "                early_hab = 0.0\n",
    "                late_hab = []\n",
    "                \n",
    "                #Reset decoder for every object (i.e., make it like a fresh hab session)\n",
    "                #Create decoder\n",
    "                decoder = nn.Sequential(nn.ReLU())\n",
    "                convT2d = nn.ConvTranspose2d(in_feat, 3, 224)\n",
    "                #this is  a better initialization ReLu/MSE Loss\n",
    "                torch.nn.init.kaiming_uniform_(convT2d.weight, a=0, mode='fan_in', nonlinearity='relu') \n",
    "                decoder.add_module('1', convT2d)\n",
    "                decoder = decoder.cuda()\n",
    "                decoder.eval()\n",
    "                decoder.train()\n",
    "                \n",
    "                #set up optimzer\n",
    "                #optimizer = torch.optim.SGD(decoder.parameters(), lr=0.01, momentum=0.9)\n",
    "                optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01)\n",
    "                for ep in range(0,epochs):\n",
    "                    train_loss = 0.0 \n",
    "                    total_loss =0.0\n",
    "                    n = 0\n",
    "                    for frames in trainloader:\n",
    "                        frames = frames.cuda()\n",
    "                        \n",
    "                        #if modelType[mm] == 'CorNet_Z' or modelType[mm] == 'CorNet_S':\n",
    "                        #    _model_feats = []\n",
    "                        #    encoder(frames)\n",
    "                        #    encode_out = _model_feats[0]\n",
    "                        #    #print(encode_out.shape)\n",
    "                        #elif modelType[mm] == 'AlexNet_SN' or modelType[mm] == 'AlexNet_IN':\n",
    "                        #    encode_out = encoder(frames) #Get encoder features\n",
    "                        #    encode_out = encode_out[:,:, None, None]\n",
    "                        #else:\n",
    "                        encode_out = encoder(frames) #Get encoder features\n",
    "                        \n",
    "                        optimizer.zero_grad() #zero out gradients from previous epoch\n",
    "                        \n",
    "                        decode_out = decoder(encode_out) #Run features through decoder\n",
    "                                                \n",
    "                        loss = criterion(decode_out, frames) #Calculate loss\n",
    "\n",
    "                        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                        loss.backward()\n",
    "                        # perform a single optimization step (parameter update)\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                        train_loss += (loss.item()*frames.size(0))\n",
    "                        n = n +1\n",
    "                        #print(train_loss, loss.item()*frames.size(0), n)\n",
    "\n",
    "                    total_loss = train_loss/n\n",
    "\n",
    "                    if ep < hab_min:\n",
    "                        early_hab += total_loss #track loss for the first 4 trials\n",
    "                        print(ep, total_loss)\n",
    "                    elif ep >= hab_min:\n",
    "                        hab_start = early_hab / hab_min #Determine habituation criterion\n",
    "                        late_hab.append(total_loss) #add current loss to habituation\n",
    "                        hab_end = mean(late_hab[(len(late_hab)-4):len(late_hab)]) #calcualte mean of last 4 hab trials\n",
    "\n",
    "                        print(ep, total_loss, hab_start, hab_end)\n",
    "                        if hab_end < (hab_start/2) and ep >= int(hab_min *2): #test if habituated\n",
    "                            break\n",
    "                \n",
    "                hab_data[hn,0] =  modelType[mm]\n",
    "                hab_data[hn,1] =  skel[ee][sk]\n",
    "                hab_data[hn,2] =  sf\n",
    "                hab_data[hn,3] =  ep\n",
    "                hab_data[hn,4] =  hab_start\n",
    "                hab_data[hn,5] =  hab_end\n",
    "                \n",
    "                print('Saving model', modelType[mm], f'Figure_{skel[ee][sk]}_{sf}', ep, hab_start, hab_end)                \n",
    "                torch.save(decoder.state_dict(), f'Weights/decoder/{exp[ee]}_{modelType[mm]}_Figure_{skel[ee][sk]}_{sf}.pt')\n",
    "                np.save(f'Weights/decoder/{exp[ee]}_Summary.npy', hab_data)\n",
    "                hn = hn + 1\n",
    "                del decoder\n",
    "    del encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on is image preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on is image preview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
